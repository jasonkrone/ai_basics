{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "724b610d-8b80-4bff-b79a-db2f0781277a",
   "metadata": {},
   "source": [
    "## Misc from the Essential Interview\n",
    "\n",
    "- [ ] Flash Attn https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad\n",
    "    - Will be good to know the key takeaway from it\n",
    "        - Mem efficient O(N) in terms of memory efficiency << I think that makes space complexity\n",
    "        - Vanilla attention is memory bound \n",
    "        - There's a memory hierarchy on GPUs\n",
    "            - SRAM (on GPU)\n",
    "            - HBM (on GPU)\n",
    "            - DRAM (on CPU)\n",
    "        - Via kernel fusion flash attn combines multiple ops from attn and only loads from HBM once\n",
    "        - Flash attn leverages both\n",
    "            - Tiling (chunking softmax scores / matrices into blocks)\n",
    "            - Recomputation in bkwrd pass\n",
    "- [ ] Minhash - think it relates to jaccard distance \n",
    "    - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.MinHashLSH.html\n",
    "- [ ] Local vs. global attn\n",
    "    - How does this work ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53fc8421-7fe1-41e3-943e-629b1847d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6f86187d-ac12-4d1f-ade0-0fc139fcbf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attn(Q, K, V, M):\n",
    "    T, D = Q.shape\n",
    "    # 4 b/c there's q, k, v, & o\n",
    "    # d b/c each of the vectors are d-dim\n",
    "    # block size for K & V\n",
    "    b_col = math.ceil(M / (4 * D))\n",
    "    # block size for Q & O \n",
    "    b_row = min(b_col, D)\n",
    "    \n",
    "    # num_blocks for Q & O & l & m \n",
    "    T_row = math.ceil(T / b_row)\n",
    "    # num_blocks for for K, V \n",
    "    T_col = math.ceil(T / b_col)\n",
    "\n",
    "    # init output on HBM \n",
    "    O = torch.zeros_like(Q)\n",
    "    # holds the cum denom for the softmax\n",
    "    l = torch.zeros(T)\n",
    "    # holds the row-wise max scores\n",
    "    m = torch.full((T,), float(\"-inf\"))\n",
    "\n",
    "    z = 1 / math.sqrt(D)\n",
    "\n",
    "    # loop through num key / value blocks\n",
    "    for j in range(T_col):\n",
    "        # load K_j, V_j from HBM to SRAM \n",
    "        start_col = j*b_col\n",
    "        end_col = j*b_col + b_col\n",
    "        K_j = K[start_col:end_col, :]\n",
    "        V_j = V[start_col:end_col, :]\n",
    "    \n",
    "        for i in range(T_row):\n",
    "            start_row = i*b_row\n",
    "            end_row = i*b_row + b_row\n",
    "            # load Q_i, O_i, l_i, m_i from HBM to SRAM\n",
    "            Q_i = Q[start_row:end_row, :]\n",
    "            O_i = O[start_row:end_row, :]\n",
    "            l_i = l[start_row:end_row]\n",
    "            m_i = m[start_row:end_row]\n",
    "    \n",
    "            scores_ij = torch.matmul(Q_i, K_j.T) * z\n",
    "            # dim 1 b/c we want the max acoss cols \n",
    "            m_ij, _ = torch.max(scores_ij, dim=1)\n",
    "            # subtract the max from the attn scores for numerical stability\n",
    "            scores_ij = torch.exp(scores_ij - m_ij[:, None])\n",
    "            # denom of softmax \n",
    "            l_ij = torch.sum(scores_ij, dim=1)\n",
    "            # update the max of the scores \n",
    "            m_i_new, _ = torch.max(torch.cat([m_i[:, None], m_ij[:, None]], dim=1), dim=1)\n",
    "            # 1st term updates max to m_i_new for l_i's, 2nd updates max to m_i_new for l_ij\n",
    "            l_i_new = torch.exp(m_i - m_i_new)*l_i + torch.exp(m_ij - m_i_new)*l_ij\n",
    "            l_i_diag = torch.diag(l_i) * torch.exp(m_i - m_i_new)\n",
    "            # write O_i to HBM\n",
    "            O_i = torch.diag(l_i_new).inverse() @ (l_i_diag @ O_i + torch.exp(m_ij - m_i_new) * scores_ij @ V_j)\n",
    "            O[start_row:end_row, :] = O_i\n",
    "            # write l_i & m_i to HBM\n",
    "            l[start_row:end_row] = l_i_new\n",
    "            m[start_row:end_row] = m_i_new\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f2c3ade8-24af-4f4f-8f18-8c1260130ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 4\n",
    "d_model = 10\n",
    "sram_size = 4 * 10 * 10\n",
    "\n",
    "# these are on HBM when passed in \n",
    "q = torch.rand((seq_len, d_model))\n",
    "k = torch.rand((seq_len, d_model))\n",
    "v = torch.rand((seq_len, d_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a342c11c-c0a2-49db-a398-a537aa9cb51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_out = F.scaled_dot_product_attention(q[None, None, ...], k[None, None, ...], v[None, None, ...], is_causal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9183cfde-e66a-496b-9d22-dd299a6a62ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i, j: 0 0\n"
     ]
    }
   ],
   "source": [
    "flash_out = flash_attn(q, k, v, sram_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4aee97e0-b211-485d-aa2e-73c61f9b53a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(gold_out, flash_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9aae7-9515-49c6-8536-9990e28f8c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009d2a1-2ef9-46ea-b136-a67b31352b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848d4af-b560-47ba-aefe-a9e5bff6a48e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpt",
   "language": "python",
   "name": "jpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
