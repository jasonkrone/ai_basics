{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396267d7-4390-46e1-b322-75accc63e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "087ad6f1-9a2b-4f8a-8df0-529f1967b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf29616-3d7e-4910-97e0-1dd3809f81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, input_ids, n_tokens, seq_lens, pad_token_id, temperature=1):\n",
    "    \"\"\" \n",
    "    model: transformers model\n",
    "    input_ids: (N, T) right padded \n",
    "    n_tokens: numm tokens to generate\n",
    "    sample_fn: takes as input logits and returns as output a sample from them \n",
    "\n",
    "    ---------------------------------------\n",
    "    anything you'd want to implement diff can be implemented as a logits processing fn\n",
    "\n",
    "    TODO should use KVCache. \n",
    "        - there's a past_key_values elem in the out_dict that could help\n",
    "    \"\"\"    \n",
    "    # init output\n",
    "    N, T = input_ids.shape\n",
    "    \n",
    "    max_len = torch.amax(seq_lens)\n",
    "    min_len = torch.amin(seq_lens)\n",
    "    \n",
    "    out_ids = torch.ones(N, max_len + n_tokens, dtype=input_ids.dtype) * pad_token_id\n",
    "    out_ids[:, :T] = input_ids\n",
    "    is_pad_mask = out_ids == pad_token_id\n",
    "        \n",
    "    i = min_len\n",
    "    # num generated tokens < n_tokens \n",
    "    is_unfinished = i - seq_lens < n_tokens\n",
    "\n",
    "    while torch.any(is_unfinished):\n",
    "        out_dict = model(input_ids=out_ids[:, :i])\n",
    "        probs = F.softmax(out_dict[\"logits\"] / temperature, dim=-1)\n",
    "        pred_ids = torch.multinomial(probs[:, -1], num_samples=1)\n",
    "        # next token is set to pad token if we've finished generating for this sequence\n",
    "        # this is a waste b/c we're doing the forward pass anyway but we'd need variable n_tokens to support\n",
    "        next_token = is_unfinished * pred_ids.view(-1) + ~is_unfinished * pad_token_id\n",
    "        # updates the values that are set to padding and keeps the values that are not padding \n",
    "        out_ids[:, i] = is_pad_mask[:, i] * next_token + ~is_pad_mask[:, i] * out_ids[:, i]\n",
    "        \n",
    "        i += 1\n",
    "        is_unfinished = i - seq_lens < n_tokens\n",
    "    \n",
    "    out_dict = model(out_ids)\n",
    "    probs = F.softmax(out_dict[\"logits\"] / temperature, dim=-1)\n",
    "    return out_ids, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "940f4d41-9db9-4fc3-910b-e79ee6eebbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative_step(m_target, m_draft, prefix_ids, pad_token_id, gamma=10, temperature=1, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Implements a speculative decoding step\n",
    "\n",
    "    m_target: \n",
    "        model we want to sample from \n",
    "    m_draft: \n",
    "        draft model we use to speculate on tokens to decode\n",
    "    prefix_ids: \n",
    "        (N, T) prefix ids -- TODO figure out if we need to pad those some way \n",
    "    pad_token_id: \n",
    "        int id of padding token\n",
    "    gamma: \n",
    "        int number of draft tokens to generate\n",
    "    \"\"\"\n",
    "    N_batch, T_prefix = prefix_ids.shape\n",
    "    seq_lens = torch.sum(prefix_ids != pad_token_id, dim=1)\n",
    "    \n",
    "    # draft ids contain the prefix as well \n",
    "    # \n",
    "    # draft_ids: (N, T_prefix+gamma)\n",
    "    # draft_probs: (N, T_prefix+gamma, V)\n",
    "    draft_ids, draft_probs = generate(\n",
    "        model=m_draft, input_ids=prefix_ids, n_tokens=gamma, seq_lens=seq_lens, \n",
    "        pad_token_id=pad_token_id, temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # target_out: (N, T_prefix+gamma)\n",
    "    target_out = m_target(draft_ids)\n",
    "    # target_probs: (N, T_prefix+gamma, V)\n",
    "    target_probs = F.softmax(target_out[\"logits\"] / temperature, dim=-1)\n",
    "    \n",
    "    # [1:]  b/c there's no probability predicted for the first input id \n",
    "    # [:-1] b/c we haven't sampled a token for position T_prefix + gamma + 1\n",
    "    # (N,  T_prefix+gamma-1)\n",
    "    m_draft_prob_of_draft = draft_probs[:, :-1].gather(2, draft_ids[:, 1:, None]).squeeze(2)    \n",
    "    m_target_prob_of_draft = target_probs[:, :-1].gather(2, draft_ids[:, 1:, None]).squeeze(2)\n",
    "    \n",
    "    draft_idxs = seq_lens[:, None] + torch.arange(gamma).expand(N_batch, gamma) - 1\n",
    "    batch_idxs = torch.arange(N_batch)[:, None]\n",
    "    \n",
    "    # take only the last gamma target probs which are for the generated tokens\n",
    "    # (N, gamma) \n",
    "    m_draft_prob_of_draft = m_draft_prob_of_draft[batch_idxs, draft_idxs]\n",
    "    m_target_prob_of_draft = m_target_prob_of_draft[batch_idxs, draft_idxs]\n",
    "    \n",
    "    # get uniform probabilities to make the accept / reject decision\n",
    "    uniform = torch.zeros_like(m_target_prob_of_draft, dtype=torch.float).uniform_(0, 1)\n",
    "    do_reject = uniform > (m_target_prob_of_draft / (m_draft_prob_of_draft + eps))\n",
    "    \n",
    "    # get the indicies where we first reject the draft models preds\n",
    "    # set to -1 if we don't reject any \n",
    "    first_true_idxs = torch.argmax(do_reject.int(), dim=1)\n",
    "    no_true_mask = ~do_reject.any(dim=1)\n",
    "    # we set this to gamma b/c that's 1+max_value\n",
    "    first_true_idxs[no_true_mask] = gamma\n",
    "\n",
    "    # create tensor for output\n",
    "    max_out_len = torch.amax(seq_lens + first_true_idxs + 1)\n",
    "    out_ids = draft_ids.clone()[:, :max_out_len]\n",
    "    # overwrite out_ids from first_true_idx onwards to pad token    \n",
    "    do_pad = torch.arange(max_out_len).expand(N_batch, max_out_len) >= seq_lens[:, None] + first_true_idxs[:, None]\n",
    "    out_ids = do_pad * pad_token_id + ~do_pad * out_ids\n",
    "\n",
    "    # (N, gamma)\n",
    "    adjusted_probs = F.relu(target_probs[batch_idxs, draft_idxs] - draft_probs[batch_idxs, draft_idxs])\n",
    "    # normalize probs to sum to 1\n",
    "    adjusted_probs = adjusted_probs / torch.sum(adjusted_probs, dim=-1, keepdim=True)\n",
    "    \n",
    "    # (N, gamma+1)\n",
    "    # add probs from target on to adjusted so that if first true idx is gamma they are chosen\n",
    "    first_true_idx_offset = (seq_lens + first_true_idxs)[:, None]\n",
    "    adjusted_probs = torch.cat([adjusted_probs, target_probs[batch_idxs, first_true_idx_offset]], dim=1)\n",
    "    \n",
    "    # (N, 1)    \n",
    "    adjusted_probs = adjusted_probs[batch_idxs, first_true_idxs[:, None]]\n",
    "    adjusted_ids = torch.multinomial(adjusted_probs.squeeze(1), num_samples=1)\n",
    "    out_ids[batch_idxs, first_true_idx_offset] = adjusted_ids\n",
    "\n",
    "    return out_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b356040c-9daf-4421-94f9-dd0dda95227e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roneneldan/TinyStories-1M\")\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "583f9a42-ffb2-4b4d-af16-1e3eaf73b38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_q = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1M\")\n",
    "m_p = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-33M\")\n",
    "\n",
    "m_q.resize_token_embeddings(len(tokenizer))\n",
    "m_p.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e7461251-3957-4cd0-aa91-a52bff58cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = tokenizer(\n",
    "    [\"once upon\", \"once\", \"once upon a time there was\"], \n",
    "    padding=\"longest\", \n",
    "    padding_side=\"right\",\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5af9991f-ed59-4caf-b895-a8013b22971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ids = input_dict[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ebc34412-809f-4c1f-a336-7341c5cb5917",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 0\n",
    "while pre_ids.shape[1] < 100:\n",
    "    n_iters += 1\n",
    "    pre_ids = speculative_step(m_p, m_q, pre_ids, tokenizer.pad_token_id, gamma=10, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "76bb8ac4-2838-4231-b09c-e38b96ef0d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['once upon a time, there was a beautiful chest. In the chest there were two special things that made them happy. One special thingful was a little lion cub. It was given a big happy roar.\\n\\nThe other cub was called Leo. Leo always liked to be stuck, especially in the dark frame. One day, Leo wanted to open the frame, but momma gave him the frame, and made an even bigger roar.\\n\\nBut Leo was feeling weak and tired. He tried',\n",
       " 'once upon a time there was an ugly bench. A lived in a park and one day a patch of grassy mud appeared on the bench. Two frogs stepped out of the mud. One of them was very colourful and the other was made out of bright stones.\\n\\nThe jumped onto the bench and they bowed to each other. The seventh frog bowed back and then swam away. The bench smiled at the birds who sang in<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>',\n",
       " \"once upon a time there was a lamp. Every day it would soak in the sun's rays. \\n\\nOne day the lamp got too hot and started to rain. The rain made it grow but it was too late. The lamp dried out before it made a hole in the wall.\\n\\nThe next day something amazing happened! A kind man removed some pumpkins from the garden and used them to turn off the lamp. Gradually he was<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>\"]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(pre_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9211b-8c2e-4918-8cbb-d9f0740f262d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e1efd-9a50-4e59-ad85-4d043d507781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpt",
   "language": "python",
   "name": "jpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
