{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ddde9b2-8f86-44b9-a7c5-c5879727c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f4cd3d02-7d2e-4a8d-b2ce-732dfbe79906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6b3c12-ed66-4bf7-a4c9-17acc29cdfe8",
   "metadata": {},
   "source": [
    "## Things that would be good to implement\n",
    "\n",
    "1. standard attention\n",
    "2. multi-query attention\n",
    "3. group-query attention << means fewer k & v\n",
    "4. ring attention\n",
    "5. linear attention \n",
    "6. rope\n",
    "7. speculative decoding\n",
    "8. KNN\n",
    "9. k-means\n",
    "10. different types of parallelism e.g., FSDP, data parallelism, model parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eaf80d-474d-41a9-986b-c87460ba0df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8574495b-88d7-4e0d-b32f-db178a5a096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Initializer(object):\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if x.ndim > 1:\n",
    "            # init to a truncated normal dist\n",
    "            x[:] = self.sample_from_truncated_normal(x.size).reshape(x.shape)\n",
    "        else:\n",
    "            # biases get initialized to zero\n",
    "            x[:] = np.zeros_like(x)\n",
    "        \n",
    "    def sample_from_truncated_normal(self, size, stdev=0.02):\n",
    "        min_value = -3 * stdev\n",
    "        max_value = 3 * stdev\n",
    "        truncated_normal = np.random.normal(0, stdev, size)\n",
    "        oob_idxs = (truncated_normal > max_value) | (truncated_normal < min_value)\n",
    "        num_oob = sum(oob_idxs)\n",
    "        while num_oob:\n",
    "            truncated_normal[oob_idxs] = np.random.normal(0, stdev, num_oob)\n",
    "            oob_idxs = (truncated_normal > max_value) | (truncated_normal < min_value)\n",
    "            num_oob = sum(oob_idxs)\n",
    "        return truncated_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a535bfb4-337a-4d2c-b65e-dd210aaf400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "\n",
    "    def __init__(self, **param_kwargs):\n",
    "        self.grads  = {}\n",
    "        self.params = {name: param for name, param in param_kwargs.items()}\n",
    "        self.initializer = initializer=Initializer()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for x in self.params.values():\n",
    "            self.initializer(x)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = self.forward(x)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Transforms the input and then returns the output\n",
    "        \"\"\"        \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, dl_out):\n",
    "        \"\"\"\n",
    "        Updates self.grads with the gradients of each parameter w.r.t. the loss and\n",
    "        returns the derivative of the input w.r.t. the loss  \n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "14e96d87-e518-49c3-9b4e-8c497ed9b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        kwargs = {}\n",
    "        kwargs[\"weight\"] = np.zeros((out_features, in_features))\n",
    "        if bias:\n",
    "            kwargs[\"bias\"] = np.zeros(out_features)\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, in_features)\n",
    "\n",
    "        Returns: (B, out_features)\n",
    "        \"\"\"\n",
    "        out = np.dot(x, self.params[\"weight\"].T)\n",
    "        if \"bias\" in self.params:\n",
    "            out += self.params[\"bias\"]\n",
    "        return out\n",
    "\n",
    "    def backward(self, x, d_up):\n",
    "        \"\"\"\n",
    "        x: (B, in_features) input to this layer \n",
    "        d_up: (B, out_features) derivative of the Loss w.r.t. the output of this module\n",
    "\n",
    "        Returns: the derivative of the output w.r.t. the input X\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: looks like we should be using a cache for backward instead of relying on the \n",
    "        # value of the params stored in the b/c \n",
    "        # those might update b4 the backward or something! \n",
    "        \n",
    "        if \"bias\" in self.params:\n",
    "            self.grads[\"bias\"] = np.sum(d_up, axis=0)\n",
    "\n",
    "        # (out_dim, in_features) = (out_features, B) x (B, in_features)\n",
    "        self.grads[\"weight\"] = np.dot(d_up.T, x)\n",
    "        # (B, in_features) = (B, out_features) x (out_features, in_features)\n",
    "        self.grads[\"in\"] = np.dot(d_up, self.params[\"weight\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c7dd1067-2568-4485-aa5a-3da2f888226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward(in_shape, get_modules):\n",
    "    np_in = np.random.rand(*in_shape).astype(np.float32)\n",
    "    tensor_in = torch.tensor(np_in)\n",
    "\n",
    "    my_module, torch_module = get_modules()\n",
    "    my_out = my_module(np_in)\n",
    "    torch_out = torch_module(tensor_in).detach().numpy()\n",
    "    print(\"outputs equal:\", np.array_equal(my_out, torch_out))\n",
    "\n",
    "\n",
    "def test_backward(in_shape, out_shape, get_modules):\n",
    "    #pass\n",
    "    my_module, torch_module = get_modules()\n",
    "\n",
    "    np_in = np.random.rand(*in_shape).astype(np.float32)\n",
    "    tensor_in = torch.tensor(np_in, requires_grad=True) # b/c we're gonna check dL/dIn\n",
    "\n",
    "    np_d_up = np.random.rand(*out_shape).astype(np.float32)\n",
    "    tensor_d_up = torch.tensor(np_d_up)\n",
    "\n",
    "    torch_out = torch_module(tensor_in)\n",
    "    # TODO: think we're passing upstream here but torch docs are sparse\n",
    "    torch_out.backward(tensor_d_up)\n",
    "    \n",
    "    my_module.backward(np_in, np_d_up)\n",
    "\n",
    "    for name, value in torch_module.named_parameters():\n",
    "        my_grad = my_module.grads[name]\n",
    "        torch_grad = value.grad.numpy()\n",
    "        print(f\"gradient of {name} is close:\", np.allclose(my_grad, torch_grad))\n",
    "\n",
    "    print(\"gradient of input is close:\", np.allclose(my_module.grads[\"in\"], tensor_in.grad.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37d02e-2ed5-4e0c-8fa8-b598ac2d168f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ee8002c5-2a87-4ea1-b593-919f60548874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lin_modules(init_kwargs, my_cls, torch_cls):\n",
    "    my_lin = my_cls(**init_kwargs)\n",
    "    torch_lin = torch_cls(**init_kwargs)\n",
    "\n",
    "    my_lin.params[\"weight\"] = torch_lin.weight.detach().numpy()\n",
    "    if torch_lin.bias is not None:\n",
    "        my_lin.params[\"bias\"] = torch_lin.bias.detach().numpy()\n",
    "    return my_lin, torch_lin\n",
    "\n",
    "\n",
    "def test_linear(n_batch, d_in, d_out, use_bias):\n",
    "    kwargs = {\n",
    "        \"in_features\": d_in,\n",
    "        \"out_features\": d_out,\n",
    "        \"bias\": use_bias,\n",
    "    }\n",
    "\n",
    "    print(f\"========= testing forward ==========\")\n",
    "    \n",
    "    test_forward(\n",
    "        in_shape=(n_batch, d_in),\n",
    "        get_modules=lambda : get_lin_modules(kwargs, Linear, nn.Linear),\n",
    "    )\n",
    "\n",
    "    print(f\"\\n========= testing backward ==========\\n\")\n",
    "    return test_backward(\n",
    "        in_shape=(n_batch, d_in),\n",
    "        out_shape=(n_batch, d_out),\n",
    "        get_modules=lambda : get_lin_modules(kwargs, Linear, nn.Linear),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "da676c7b-210d-46c7-ab0a-b8399d12f130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= testing forward ==========\n",
      "outputs equal: True\n",
      "\n",
      "========= testing backward ==========\n",
      "\n",
      "gradient of weight is close: True\n",
      "gradient of bias is close: True\n",
      "gradient of input is close: True\n"
     ]
    }
   ],
   "source": [
    "test_linear(3, 4, 2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca9f18-2f2d-4176-b0c7-46c69ec3e31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6c8fa937-e422-425f-abae-fa6191a7389f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5b1fc301-4b2a-44cc-9d18-d98bdb0c8a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0362a185-f0c0-4a09-9b4f-57cf9e95d718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "80809d08-190d-4d15-8ae2-f7104959d01c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpt",
   "language": "python",
   "name": "jpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
