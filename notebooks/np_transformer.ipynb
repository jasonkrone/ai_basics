{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ddde9b2-8f86-44b9-a7c5-c5879727c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8574495b-88d7-4e0d-b32f-db178a5a096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Initializer(object):\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if x.ndim > 1:\n",
    "            # init to a truncated normal dist\n",
    "            x[:] = self.sample_from_truncated_normal(x.size).reshape(x.shape)\n",
    "        else:\n",
    "            # biases get initialized to zero\n",
    "            x[:] = np.zeros_like(x)\n",
    "        \n",
    "    def sample_from_truncated_normal(self, size, stdev=0.02):\n",
    "        min_value = -3 * stdev\n",
    "        max_value = 3 * stdev\n",
    "        truncated_normal = np.random.normal(0, stdev, size)\n",
    "        oob_idxs = (truncated_normal > max_value) | (truncated_normal < min_value)\n",
    "        num_oob = sum(oob_idxs)\n",
    "        while num_oob:\n",
    "            truncated_normal[oob_idxs] = np.random.normal(0, stdev, num_oob)\n",
    "            oob_idxs = (truncated_normal > max_value) | (truncated_normal < min_value)\n",
    "            num_oob = sum(oob_idxs)\n",
    "        return truncated_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a535bfb4-337a-4d2c-b65e-dd210aaf400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "\n",
    "    def __init__(self, **param_kwargs):\n",
    "        self.grads  = {}\n",
    "        self.params = {name: param for name, param in param_kwargs}\n",
    "        self.initializer = initializer=Initializer()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for x in self.params.values():\n",
    "            self.initializer(x)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = self.forward(x)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Transforms the input and then returns the output\n",
    "        \"\"\"        \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, dl_out):\n",
    "        \"\"\"\n",
    "        Updates self.grads with the gradients of each parameter w.r.t. the loss and\n",
    "        returns the derivative of the input w.r.t. the loss  \n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "14e96d87-e518-49c3-9b4e-8c497ed9b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, bias=False):\n",
    "        kwargs = {}\n",
    "        kwargs[\"w\"] = np.zeros((out_dim, in_dim))\n",
    "        if bias:\n",
    "            kwargs[\"bias\"] = np.zeros(out_dim)\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, in_dim)\n",
    "        \"\"\"\n",
    "        out = np.dot(x, self.params[\"w\"].T)\n",
    "        if \"bias\" in self.params:\n",
    "            out += self.params[\"bias\"]\n",
    "\n",
    "    def backward(self, dl_out):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c697ebb3-5382-4dbb-abda-26c7486463f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpt",
   "language": "python",
   "name": "jpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
