{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ddde9b2-8f86-44b9-a7c5-c5879727c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4cd3d02-7d2e-4a8d-b2ce-732dfbe79906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8574495b-88d7-4e0d-b32f-db178a5a096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Initializer(object):\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if x.ndim > 1:\n",
    "            # init to a truncated normal dist\n",
    "            x[:] = self.sample_from_truncated_normal(x.size).reshape(x.shape)\n",
    "        else:\n",
    "            # biases get initialized to zero\n",
    "            x[:] = np.zeros_like(x)\n",
    "        \n",
    "    def sample_from_truncated_normal(self, size, stdev=0.02):\n",
    "        min_value = -3 * stdev\n",
    "        max_value = 3 * stdev\n",
    "        truncated_normal = np.random.normal(0, stdev, size)\n",
    "        oob_idxs = (truncated_normal > max_value) | (truncated_normal < min_value)\n",
    "        num_oob = sum(oob_idxs)\n",
    "        while num_oob:\n",
    "            truncated_normal[oob_idxs] = np.random.normal(0, stdev, num_oob)\n",
    "            oob_idxs = (truncated_normal > max_value) | (truncated_normal < min_value)\n",
    "            num_oob = sum(oob_idxs)\n",
    "        return truncated_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a535bfb4-337a-4d2c-b65e-dd210aaf400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "\n",
    "    def __init__(self, **param_kwargs):\n",
    "        self.grads  = {}\n",
    "        self.params = {name: param for name, param in param_kwargs.items()}\n",
    "        self.initializer = initializer=Initializer()\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for x in self.params.values():\n",
    "            self.initializer(x)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = self.forward(x)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Transforms the input and then returns the output\n",
    "        \"\"\"        \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, dl_out):\n",
    "        \"\"\"\n",
    "        Updates self.grads with the gradients of each parameter w.r.t. the loss and\n",
    "        returns the derivative of the input w.r.t. the loss  \n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "14e96d87-e518-49c3-9b4e-8c497ed9b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, bias=False):\n",
    "        kwargs = {}\n",
    "        kwargs[\"w\"] = np.zeros((out_dim, in_dim))\n",
    "        if bias:\n",
    "            kwargs[\"bias\"] = np.zeros(out_dim)\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, in_dim)\n",
    "\n",
    "        Returns: (B, out_dim)\n",
    "        \"\"\"\n",
    "        out = np.dot(x, self.params[\"w\"].T)\n",
    "        if \"bias\" in self.params:\n",
    "            out += self.params[\"bias\"]\n",
    "        return out\n",
    "\n",
    "    def backward(self, x, d_up):\n",
    "        \"\"\"\n",
    "        x: (B, in_dim). input to this layer \n",
    "        \n",
    "        d_up: (B, out_dim)\n",
    "\n",
    "        Returns: the derivative of the output w.r.t. the input X\n",
    "        \"\"\"\n",
    "        if \"bias\" in self.params:\n",
    "            self.grads[\"bias\"] = np.ones_like(self.bias)\n",
    "\n",
    "        # (out_dim, in_dim) = (out_dim, B) x (B, in_dim)\n",
    "        self.grads[\"w\"] = np.dot(d_up.T, x)\n",
    "        # (B, in_dim) = (B, out_dim) x (out_dim, in_dim)\n",
    "        self.grads[\"in\"] = np.dot(d_up, self.params[\"w\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c7dd1067-2568-4485-aa5a-3da2f888226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward(my_module, torch_module, np_in):\n",
    "    tensor_in = torch.tensor(np_in)\n",
    "    my_out = my_module(np_in)\n",
    "    torch_out = torch_module(tensor_in).detach().numpy()\n",
    "    return np.array_equal(my_out, torch_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cdff1c-9710-47ee-bf60-dfee6450b1b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08cd9c8b-ef1b-4571-aab2-523f13c30961",
   "metadata": {},
   "source": [
    "### Test Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d3782-ef85-4dfb-89df-fe316ec77d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e5a29b0f-838c-41b3-9d03-fb29a074afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_linear_forward(n_batch, d_in, d_out, use_bias):\n",
    "    my_lin = Linear(d_in, d_out, bias=False)\n",
    "    torch_lin = nn.Linear(d_in, d_out, bias=False)\n",
    "    # set init weights equal\n",
    "    my_lin.params[\"w\"] = torch_lin.weight.detach().numpy()\n",
    "    # create dummy input\n",
    "    x_in = np.random.rand(n_batch, d_in).astype(np.float32)\n",
    "    return test_forward(my_lin, torch_lin, x_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983b86b-70f3-4381-bd66-f021b0ce4bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d7f8f8da-b607-4fc1-8c88-0c76c3ccb049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_linear_forward(3, 4, 2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b898b828-f5a6-41ee-b35f-de9d116be57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_lin.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93403c-ea63-46c9-adeb-dcf0d95fbc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80dad6-74ea-41c8-8a84-c4f5ae04918d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6c8fa937-e422-425f-abae-fa6191a7389f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5b1fc301-4b2a-44cc-9d18-d98bdb0c8a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0362a185-f0c0-4a09-9b4f-57cf9e95d718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "80809d08-190d-4d15-8ae2-f7104959d01c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c697ebb3-5382-4dbb-abda-26c7486463f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpt",
   "language": "python",
   "name": "jpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
