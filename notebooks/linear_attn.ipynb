{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6836f9-050a-4f5a-956e-bdcf32e8f26d",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "https://github.com/idiap/fast-transformers/tree/2ad36b97e64cb93862937bd21fcc9568d989561f/fast_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c5caab-08f4-442a-a852-1cbaddf56dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2d62b-38fd-4418-bac7-80dfc5b49e65",
   "metadata": {},
   "source": [
    "### Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7de6e1a7-36e8-468c-acd3-661ef8822bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseMask(object):\n",
    "    @property\n",
    "    def bool_matrix(self):\n",
    "        \"\"\"Return a bool (uint8) matrix with 1s to all places that should be\n",
    "        kept.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def float_matrix(self):\n",
    "        \"\"\"Return the bool matrix as a float to be used as a multiplicative\n",
    "        mask for non softmax attentions.\"\"\"\n",
    "        if not hasattr(self, \"_float_matrix\"):\n",
    "            with torch.no_grad():\n",
    "                self._float_matrix = self.bool_matrix.float()\n",
    "        return self._float_matrix\n",
    "\n",
    "    @property\n",
    "    def lengths(self):\n",
    "        \"\"\"If the matrix is of the following form\n",
    "\n",
    "            1 1 1 0 0 0 0\n",
    "            1 0 0 0 0 0 0\n",
    "            1 1 0 0 0 0 0\n",
    "\n",
    "        then return it as a vector of integers\n",
    "\n",
    "            3 1 2.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"_lengths\"):\n",
    "            with torch.no_grad():\n",
    "                lengths = self.bool_matrix.long().sum(dim=-1)\n",
    "                # make sure that the mask starts with 1s and continues with 0s\n",
    "                # this should be changed to something more efficient, however,\n",
    "                # I chose simplicity over efficiency since the LengthMask class\n",
    "                # will be used anyway (and the result is cached)\n",
    "                m = self.bool_matrix.view(-1, self.shape[-1])\n",
    "                for i, l in enumerate(lengths.view(-1)):\n",
    "                    if not torch.all(m[i, :l]):\n",
    "                        raise ValueError(\"The mask is not a length mask\")\n",
    "                self._lengths = lengths\n",
    "        return self._lengths\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        \"\"\"Return the shape of the boolean mask.\"\"\"\n",
    "        return self.bool_matrix.shape\n",
    "\n",
    "    @property\n",
    "    def additive_matrix(self):\n",
    "        \"\"\"Return a float matrix to be added to an attention matrix before\n",
    "        softmax.\"\"\"\n",
    "        if not hasattr(self, \"_additive_matrix\"):\n",
    "            with torch.no_grad():\n",
    "                self._additive_matrix = torch.log(self.bool_matrix.float())\n",
    "        return self._additive_matrix\n",
    "\n",
    "    @property\n",
    "    def additive_matrix_finite(self):\n",
    "        \"\"\"Same as additive_matrix but with -1e24 instead of infinity.\"\"\"\n",
    "        if not hasattr(self, \"_additive_matrix_finite\"):\n",
    "            with torch.no_grad():\n",
    "                self._additive_matrix_finite = (\n",
    "                    (~self.bool_matrix).float() * (-1e24)\n",
    "                )\n",
    "        return self._additive_matrix_finite\n",
    "\n",
    "    @property\n",
    "    def all_ones(self):\n",
    "        \"\"\"Return true if the mask is all ones.\"\"\"\n",
    "        if not hasattr(self, \"_all_ones\"):\n",
    "            with torch.no_grad():\n",
    "                self._all_ones = torch.all(self.bool_matrix)\n",
    "        return self._all_ones\n",
    "\n",
    "    @property\n",
    "    def lower_triangular(self):\n",
    "        \"\"\"Return true if the attention is a triangular causal mask.\"\"\"\n",
    "        if not hasattr(self, \"_lower_triangular\"):\n",
    "            self._lower_triangular = False\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    lengths = self.lengths\n",
    "                    if len(lengths.shape) == 1:\n",
    "                        target = torch.arange(\n",
    "                            1,\n",
    "                            len(lengths)+1,\n",
    "                            device=lengths.device\n",
    "                        )\n",
    "                        self._lower_triangular = torch.all(lengths == target)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        return self._lower_triangular\n",
    "\n",
    "\n",
    "class FullMask(BaseMask):\n",
    "    \"\"\"Thin wrapper over a pytorch tensor that provides the BaseMask\n",
    "    interface.\n",
    "\n",
    "    The arguments can be given both by keyword arguments and positional\n",
    "    arguments. To imitate function overloading, the constructor checks the type\n",
    "    of the first argument and if it is a tensor it treats it as the mask.\n",
    "    otherwise it assumes that it was the N argument.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        mask: The mask as a PyTorch tensor.\n",
    "        N: The rows of the all True mask to be created if the mask argument is\n",
    "           not provided.\n",
    "        M: The columns of the all True mask to be created if the mask argument\n",
    "           is not provided. If N is given M defaults to N.\n",
    "        device: The device to create the mask in (defaults to cpu)\n",
    "    \"\"\"\n",
    "    def __init__(self, mask=None, N=None, M=None, device=\"cpu\"):\n",
    "        # mask is a tensor so we ignore N and M\n",
    "        if mask is not None and isinstance(mask, torch.Tensor):\n",
    "            if mask.dtype != torch.bool:\n",
    "                raise ValueError(\"FullMask expects the mask to be bool\")\n",
    "            with torch.no_grad():\n",
    "                self._mask = mask.clone()\n",
    "            return\n",
    "\n",
    "        # mask is an integer, N is an integer and M is None so assume they were\n",
    "        # passed as N, M\n",
    "        if mask is not None and M is None and isinstance(mask, int):\n",
    "            M = N\n",
    "            N = mask\n",
    "\n",
    "        if N is not None:\n",
    "            M = M or N\n",
    "            with torch.no_grad():\n",
    "                self._mask = torch.ones(N, M, dtype=torch.bool, device=device)\n",
    "            self._all_ones = True\n",
    "            return\n",
    "\n",
    "        raise ValueError(\"Either mask or N should be provided\")\n",
    "\n",
    "    @property\n",
    "    def bool_matrix(self):\n",
    "        return self._mask\n",
    "\n",
    "\n",
    "class LengthMask(BaseMask):\n",
    "    \"\"\"Provide a BaseMask interface for lengths. Mostly to be used with\n",
    "    sequences of different lengths.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        lengths: The lengths as a PyTorch long tensor\n",
    "        max_len: The maximum length for the mask (defaults to lengths.max())\n",
    "        device: The device to be used for creating the masks (defaults to\n",
    "                lengths.device)\n",
    "    \"\"\"\n",
    "    def __init__(self, lengths, max_len=None, device=None):\n",
    "        self._device = device or lengths.device\n",
    "        with torch.no_grad():\n",
    "            self._lengths = lengths.clone().to(self._device)\n",
    "        self._max_len = max_len or self._lengths.max()\n",
    "\n",
    "        self._bool_matrix = None\n",
    "        self._all_ones = torch.all(self._lengths == self._max_len).item()\n",
    "\n",
    "    @property\n",
    "    def bool_matrix(self):\n",
    "        if self._bool_matrix is None:\n",
    "            with torch.no_grad():\n",
    "                indices = torch.arange(self._max_len, device=self._device)\n",
    "                self._bool_matrix = (\n",
    "                    indices.view(1, -1) < self._lengths.view(-1, 1)\n",
    "                )\n",
    "        return self._bool_matrix\n",
    "\n",
    "\n",
    "class TriangularCausalMask(LengthMask):\n",
    "    \"\"\"A square matrix with everything masked out above the diagonal.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        N: The size of the matrix\n",
    "        device: The device to create the mask in (defaults to cpu)\n",
    "    \"\"\"\n",
    "    def __init__(self, N, device=\"cpu\"):\n",
    "        lengths = torch.arange(1, N+1, device=device)\n",
    "        super(TriangularCausalMask, self).__init__(lengths, N, device)\n",
    "        self._lower_triangular = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc860e-b428-4a47-914f-af843881e618",
   "metadata": {},
   "source": [
    "### ELU Feature Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7298fee-bf61-4163-907f-811e1201406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class FeatureMap(Module):\n",
    "    \"\"\"Define the FeatureMap interface.\"\"\"\n",
    "    def __init__(self, query_dims):\n",
    "        super().__init__()\n",
    "        self.query_dims = query_dims\n",
    "\n",
    "    def new_feature_map(self, device):\n",
    "        \"\"\"Create a new instance of this feature map. In particular, if it is a\n",
    "        random feature map sample new parameters.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward_queries(self, x):\n",
    "        \"\"\"Encode the queries `x` using this feature map.\"\"\"\n",
    "        return self(x)\n",
    "\n",
    "    def forward_keys(self, x):\n",
    "        \"\"\"Encode the keys `x` using this feature map.\"\"\"\n",
    "        return self(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Encode x using this feature map. For symmetric feature maps it\n",
    "        suffices to define this function, but for asymmetric feature maps one\n",
    "        needs to define the `forward_queries` and `forward_keys` functions.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def factory(cls, *args, **kwargs):\n",
    "        \"\"\"Return a function that when called with the query dimensions returns\n",
    "        an instance of this feature map.\n",
    "\n",
    "        It is inherited by the subclasses so it is available in all feature\n",
    "        maps.\n",
    "        \"\"\"\n",
    "        def inner(query_dims):\n",
    "            return cls(query_dims, *args, **kwargs)\n",
    "        return inner\n",
    "\n",
    "\n",
    "class ActivationFunctionFeatureMap(FeatureMap):\n",
    "    \"\"\"Define a feature map that is simply an element-wise activation\n",
    "    function.\"\"\"\n",
    "    def __init__(self, query_dims, activation_function):\n",
    "        super().__init__(query_dims)\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def new_feature_map(self, device):\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation_function(x)\n",
    "\n",
    "\n",
    "elu_feature_map = ActivationFunctionFeatureMap.factory(\n",
    "    lambda x: torch.nn.functional.elu(x) + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d60256-ac8f-4dad-98c4-4fd285ec583c",
   "metadata": {},
   "source": [
    "### Linear Attention Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b232073-9484-4c5d-bd15-1d8edf960881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttention(Module):\n",
    "    \"\"\"Implement unmasked attention using dot product of feature maps in\n",
    "    O(N D^2) complexity.\n",
    "\n",
    "    Given the queries, keys and values as Q, K, V instead of computing\n",
    "\n",
    "        V' = softmax(Q.mm(K.t()), dim=-1).mm(V),\n",
    "\n",
    "    we make use of a feature map function Φ(.) and perform the following\n",
    "    computation\n",
    "\n",
    "        V' = normalize(Φ(Q).mm(Φ(K).t())).mm(V).\n",
    "\n",
    "    The above can be computed in O(N D^2) complexity where D is the\n",
    "    dimensionality of Q, K and V and N is the sequence length. Depending on the\n",
    "    feature map, however, the complexity of the attention might be limited.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        feature_map: callable, a callable that applies the feature map to the\n",
    "                     last dimension of a tensor (default: elu(x)+1)\n",
    "        eps: float, a small number to ensure the numerical stability of the\n",
    "             denominator (default: 1e-6)\n",
    "        event_dispatcher: str or EventDispatcher instance to be used by this\n",
    "                          module for dispatching events (default: the default\n",
    "                          global dispatcher)\n",
    "    \"\"\"\n",
    "    def __init__(self, query_dimensions, feature_map=None, eps=1e-6, event_dispatcher=\"\"):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        self.feature_map = elu_feature_map(query_dimensions)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, query_lengths, key_lengths):\n",
    "        \"\"\"\n",
    "        queries: (N, L, H, E)\n",
    "        keys: (N, S, H, E)\n",
    "        values: (N, S, H, D)\n",
    "        attn_mask: (L, S)\n",
    "        query_lengths: (N, L)\n",
    "        key_lengths: (N, S)\n",
    "\n",
    "        where\n",
    "            - N: batch-size\n",
    "            - L: seq len for queries\n",
    "            - S: seq len for keys & values\n",
    "            - H: number of heads\n",
    "            - E: key & query dim\n",
    "            - D: value dim\n",
    "        \"\"\"\n",
    "        # Apply the feature map to the queries and keys\n",
    "        self.feature_map.new_feature_map(queries.device)\n",
    "        Q = self.feature_map.forward_queries(queries)\n",
    "        K = self.feature_map.forward_keys(keys)\n",
    "\n",
    "        # Apply the key padding mask and make sure that the attn_mask is\n",
    "        # all_ones\n",
    "        if not attn_mask.all_ones:\n",
    "            raise RuntimeError((\"LinearAttention does not support arbitrary attention masks\"))\n",
    "\n",
    "        # key_lengths => (N, S, 1, 1)\n",
    "        K = K * key_lengths.float_matrix[:, :, None, None]\n",
    "\n",
    "        # Compute the KV matrix, namely the dot product of keys and values so\n",
    "        # that we never explicitly compute the attention matrix and thus\n",
    "        # decrease the complexity\n",
    "        KV = torch.einsum(\"nshd,nshm->nhmd\", K, values)\n",
    "\n",
    "        # Compute the normalizer\n",
    "        Z = 1/(torch.einsum(\"nlhd,nhd->nlh\", Q, K.sum(dim=1))+self.eps)\n",
    "\n",
    "        # Finally compute and return the new values\n",
    "        V = torch.einsum(\"nlhd,nhmd,nlh->nlhm\", Q, KV, Z)\n",
    "\n",
    "        return V.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40faa5ce-cda5-4ac0-8abb-bea6027be1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TestLinearAttention(object): #(unittest.TestCase):\n",
    "    \n",
    "    def _get_inputs(self, N=10, L=5, S=8, H=4, E=32, D=64, device=\"cpu\"):\n",
    "        return (\n",
    "            torch.rand(N, L, H, E).to(device), # Q\n",
    "            torch.rand(N, S, H, E).to(device), # K\n",
    "            torch.rand(N, S, H, D).to(device), # V\n",
    "            FullMask(L, S, device=device), # m1\n",
    "            FullMask(N, L, device=device), # m2\n",
    "            FullMask(N, S, device=device) # m3\n",
    "        )\n",
    "\n",
    "    # TODO: JPK added\n",
    "    def get_their_forward(self):\n",
    "        att = LinearAttention(32)\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs()\n",
    "        v = att(q, k, v, m1, m2, m3)\n",
    "        return v\n",
    "    \n",
    "    def test_forward(self):\n",
    "        att = LinearAttention(32)\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs()\n",
    "        v = att(q, k, v, m1, m2, m3)\n",
    "        self.assertTrue(v.is_contiguous())\n",
    "\n",
    "    def test_masking(self):\n",
    "        att = LinearAttention(32)\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs()\n",
    "\n",
    "        # Make sure that we raise an error if m1 is not all ones\n",
    "        with self.assertRaises(RuntimeError):\n",
    "            att(q, k, v, FullMask(torch.rand(*m1.shape) > 0.5), m2, m3)\n",
    "\n",
    "        # Make sure that the key lengths is paid attention to\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs(S=10, D=1)\n",
    "        m3 = LengthMask(torch.tensor(list(range(10)))+1)\n",
    "        for i in range(9):\n",
    "            v[i, i+1:] = 1e9\n",
    "        v_new = att(q, k, v, m1, m2, m3)\n",
    "        self.assertLess(v_new.max().item(), 1)\n",
    "\n",
    "    def test_benchmark_cpu(self):\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs(L=1024, S=1024, E=64, D=64)\n",
    "        att = LinearAttention(64)\n",
    "\n",
    "        # warmup the cache\n",
    "        for i in range(10):\n",
    "            v_new = att(q, k, v, m1, m2, m3)\n",
    "\n",
    "        # measure\n",
    "        start = time.time()\n",
    "        for i in range(10):\n",
    "            v_new = att(q, k, v, m1, m2, m3)\n",
    "        end = time.time()\n",
    "        print(\"CPU time taken:\", (end-start)*1000, \"(ms)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc6542bc-db50-458d-bada-c209e8094d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyLinearAttention(Module):\n",
    "    \"\"\"Implement unmasked attention using dot product of feature maps in\n",
    "    O(N D^2) complexity.\n",
    "\n",
    "    Given the queries, keys and values as Q, K, V instead of computing\n",
    "\n",
    "        V' = softmax(Q.mm(K.t()), dim=-1).mm(V),\n",
    "\n",
    "    we make use of a feature map function Φ(.) and perform the following\n",
    "    computation\n",
    "\n",
    "        V' = normalize(Φ(Q).mm(Φ(K).t())).mm(V).\n",
    "\n",
    "    The above can be computed in O(N D^2) complexity where D is the\n",
    "    dimensionality of Q, K and V and N is the sequence length. Depending on the\n",
    "    feature map, however, the complexity of the attention might be limited.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        feature_map: callable, a callable that applies the feature map to the\n",
    "                     last dimension of a tensor (default: elu(x)+1)\n",
    "        eps: float, a small number to ensure the numerical stability of the\n",
    "             denominator (default: 1e-6)\n",
    "        event_dispatcher: str or EventDispatcher instance to be used by this\n",
    "                          module for dispatching events (default: the default\n",
    "                          global dispatcher)\n",
    "    \"\"\"\n",
    "    def __init__(self, query_dimensions, feature_map=None, eps=1e-6, event_dispatcher=\"\"):\n",
    "        super(MyLinearAttention, self).__init__()\n",
    "        self.feature_map = elu_feature_map(query_dimensions)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, query_lengths, key_lengths):\n",
    "        \"\"\"\n",
    "        queries: (N, L, H, E)\n",
    "        keys: (N, S, H, E)\n",
    "        values: (N, S, H, D)\n",
    "        attn_mask: (L, S)\n",
    "        query_lengths: (N, L)\n",
    "        key_lengths: (N, S)\n",
    "\n",
    "        where\n",
    "            - N: batch-size\n",
    "            - L: seq len for queries\n",
    "            - S: seq len for keys & values\n",
    "            - H: number of heads\n",
    "            - E: key & query dim\n",
    "            - D: value dim\n",
    "        \"\"\"\n",
    "        # Apply the feature map to the queries and keys\n",
    "        self.feature_map.new_feature_map(queries.device)\n",
    "        Q = self.feature_map.forward_queries(queries)\n",
    "        K = self.feature_map.forward_keys(keys)\n",
    "        # change the shapes so we broadcast across the right dims\n",
    "        N, L, H, E = Q.shape\n",
    "        _, S, _, _ = K.shape\n",
    "\n",
    "        # (N, L, H, E) => (N, H, L, E)\n",
    "        Q = Q.transpose(1, 2)\n",
    "        # (N, S, H, E) => (N, H, S, E)\n",
    "        K = K.transpose(1, 2)\n",
    "        # (N, S, H, D) => (N, H, S, D)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # reshape K & V to get KV\n",
    "        # (N, H, S, E) => (N, H, S, E, 1)\n",
    "        K = K[:, :, :, :, None]\n",
    "        # (N, H, S, D) => (N, H, S, 1, D)\n",
    "        values = values[:, :, :, None, :]\n",
    "\n",
    "        # (N, H, S, E, 1) x (N, H, S, 1, D) = (N, H, E, D)\n",
    "        KV = torch.sum(K @ values, dim=2)\n",
    "        # (N, H, L, E) x (N, H, E, D) = (N, H, L, D) \n",
    "        QKV = Q @ KV \n",
    "        # (N, H, E)\n",
    "        K_sum = torch.sum(K, dim=2).squeeze()\n",
    "        # (N, H, L, E) x (N, H, E, 1) = > (N, H, L, 1) \n",
    "        Z = 1 / (Q @ K_sum[:, :, :, None] + self.eps).squeeze()\n",
    "        # (N, H, L, D) x (N, H, L, 1) = (N, H, L, D) \n",
    "        out = QKV * Z[:, :, :, None]\n",
    "        # (N, H, L, D) => (N, L, H, D)\n",
    "        out = out.transpose(1, 2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "49da8956-6ecb-44b2-905f-dcc4e339a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EinsumLinearAttention(Module):\n",
    "    \n",
    "    def __init__(self, query_dimensions, feature_map=None, eps=1e-6, event_dispatcher=\"\"):\n",
    "        super().__init__()\n",
    "        self.feature_map = elu_feature_map(query_dimensions)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, query_lengths, key_lengths):\n",
    "        \"\"\"\n",
    "        queries: (N, L, H, E)\n",
    "        keys: (N, S, H, E)\n",
    "        values: (N, S, H, D)\n",
    "        attn_mask: (L, S)\n",
    "        query_lengths: (N, L)\n",
    "        key_lengths: (N, S)\n",
    "\n",
    "        where\n",
    "            - N: batch-size\n",
    "            - L: seq len for queries\n",
    "            - S: seq len for keys & values\n",
    "            - H: number of heads\n",
    "            - E: key & query dim\n",
    "            - D: value dim\n",
    "        \"\"\"\n",
    "        # Apply the feature map to the queries and keys\n",
    "        self.feature_map.new_feature_map(queries.device)\n",
    "        Q = self.feature_map.forward_queries(queries)\n",
    "        K = self.feature_map.forward_keys(keys)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = values.transpose(1, 2)\n",
    "\n",
    "        # (N, H, E, D)\n",
    "        KV = torch.einsum(\"...ki,...kj->...ij\", [K, V])\n",
    "        # (N, H, L, E) x (N, H, E, D) = (N, H, L, D)\n",
    "        QKV = torch.einsum(\"...ik,...kj->...ij\", [Q, KV])\n",
    "        # (N, H, E)\n",
    "        # TODO: they do something w/ key lengths that you should look at\n",
    "        K_sum = torch.sum(K, dim=-2)\n",
    "        # (N, H, L, E) dot prod (N, H, E) = (N, H, L) \n",
    "        Z = 1 / (torch.einsum(\"...ij,...j->...i\", [Q, K_sum]) + self.eps) \n",
    "        # (N, H, L, D) * (N, H, L, 1) = (N, H, L, D)\n",
    "        out = QKV * Z[..., None]\n",
    "        #  (N, H, L, D) =>  (N, L, H, D)\n",
    "        out = out.transpose(1, 2)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f91aa-491d-470a-bfd3-09b7dd82efc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "07ce78ae-9063-40ba-9287-a435ff7a1d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_temp = torch.rand(10, 4, 5, 64)\n",
    "z_temp = torch.rand(10, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dbe45afd-c1c4-4790-9291-f88bf0f35753",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_tester = TestLinearAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "afc3c149-4b63-47b0-b30e-54e445f334b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v, m1, m2, m3 = lin_tester._get_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "41bbe50e-f319-4039-9693-e3912c3647fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mine = MyLinearAttention(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ef5619e3-a769-4836-a131-6a3d6fc395dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "theirs = LinearAttention(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5b454ee2-bf0d-4a57-b888-dd8d86974513",
   "metadata": {},
   "outputs": [],
   "source": [
    "ein = EinsumLinearAttention(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "81719ef4-72d6-4c29-838e-3abe55737a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([10, 4, 5, 32])\n",
      "V shape: torch.Size([10, 4, 8, 64])\n",
      "K shape: torch.Size([10, 4, 8, 32])\n",
      "KV shape: torch.Size([10, 4, 32, 64])\n",
      "QKV shape: torch.Size([10, 4, 5, 64])\n",
      "K sum shape: torch.Size([10, 4, 32])\n",
      "Z shape: torch.Size([10, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "my_out = mine(q, k, v, m1, m2, m3)\n",
    "their_out = theirs(q, k, v, m1, m2, m3)\n",
    "ein_out = ein(q, k, v, m1, m2, m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b66e07bb-50e5-4ca1-a923-d128ae133449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(my_out, their_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b67fe190-5b4d-4403-bdf9-b488e07bc628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(ein_out, their_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09149244-0e10-4cad-852d-3dafb51af6de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e33f0fa7-caf5-4c17-b4f3-89f319aafcc1",
   "metadata": {},
   "source": [
    "### Linear Attention Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22a454-928d-4806-ae29-f42871d7bb99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab72cd-cb01-47db-beb4-737aae4524fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f02abb-6a8e-4786-8b04-7908bc7b0156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c1df0cd-bc09-4b56-a29d-2aac1e7a42f1",
   "metadata": {},
   "source": [
    "### Causal Linear Attention Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d07f1178-6b4d-4824-8b14-9026fcbede33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalLinearAttention(Module):\n",
    "    \"\"\"Implement causally masked attention using dot product of feature maps in\n",
    "    O(N D^2) complexity.\n",
    "\n",
    "    See fast_transformers.attention.linear_attention.LinearAttention for the\n",
    "    general concept of replacing the softmax with feature maps. In addition to\n",
    "    that, we also make use of the fact that causal masking is a triangular mask\n",
    "    which allows us to apply the masking and still compute the attention in O(N\n",
    "    D^2) complexity.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        feature_map: callable, a callable that applies the feature map to the\n",
    "                     last dimension of a tensor (default: elu(x)+1)\n",
    "        eps: float, a small number to ensure the numerical stability of the\n",
    "             denominator (default: 1e-6)\n",
    "        event_dispatcher: str or EventDispatcher instance to be used by this\n",
    "                          module for dispatching events (default: the default\n",
    "                          global dispatcher)\n",
    "    \"\"\"\n",
    "    def __init__(self, query_dimensions, feature_map=None, eps=1e-6,\n",
    "                 event_dispatcher=\"\"):\n",
    "        super(CausalLinearAttention, self).__init__()\n",
    "        self.feature_map = (\n",
    "            feature_map(query_dimensions) if feature_map else\n",
    "            elu_feature_map(query_dimensions)\n",
    "        )\n",
    "        self.eps = eps\n",
    "        self.event_dispatcher = EventDispatcher.get(event_dispatcher)\n",
    "\n",
    "    def _make_sizes_compatible(self, Q, K):\n",
    "        \"\"\"Either slice or pad K in case that the sizes do not match between Q\n",
    "        and K.\"\"\"\n",
    "        N, L, H, E = Q.shape\n",
    "        _, S, _, _ = K.shape\n",
    "        if L == S:\n",
    "            return Q, K\n",
    "\n",
    "        if L < S:\n",
    "            return Q, K[:, :L, :, :]\n",
    "\n",
    "        if L > S:\n",
    "            return Q, torch.cat([K, K.new_zeros(N, L-S, H, E)], dim=1)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, query_lengths,\n",
    "                key_lengths):\n",
    "        # Apply the feature map to the queries and keys\n",
    "        self.feature_map.new_feature_map(queries.device)\n",
    "        Q = self.feature_map.forward_queries(queries)\n",
    "        K = self.feature_map.forward_keys(keys)\n",
    "\n",
    "        # Apply the key padding mask and make sure the attn_mask is a\n",
    "        # lower triangular causal mask\n",
    "        if not attn_mask.lower_triangular:\n",
    "            raise RuntimeError((\"CausalLinearAttention only supports full \"\n",
    "                                \"lower triangular masks\"))\n",
    "        K = K * key_lengths.float_matrix[:, :, None, None]\n",
    "\n",
    "        # Ensure that Q and K have compatible sizes for the following\n",
    "        # computations, namely L == S\n",
    "        Q, K = self._make_sizes_compatible(Q, K)\n",
    "\n",
    "        # TODO: Shall we divide the Q and K with a relatively large number to\n",
    "        #       avoid numerical instabilities in computing the denominator?\n",
    "        #       We used to divide each with the max norm of all q and k but\n",
    "        #       that seems relatively costly for a simple normalization.\n",
    "\n",
    "        # Compute the normalizers\n",
    "        Z = 1/(torch.einsum(\"nlhi,nlhi->nlh\", Q, K.cumsum(1)) + self.eps)\n",
    "\n",
    "        # Compute the unnormalized result\n",
    "        V = causal_linear(\n",
    "            Q,\n",
    "            K,\n",
    "            values\n",
    "        )\n",
    "\n",
    "        return V * Z[:, :, :, None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36c604-e114-44d7-8d44-64393885a01c",
   "metadata": {},
   "source": [
    "### Causal Linear Attention Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c867eb-7b2e-4d07-85b3-6209f47d841a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unittest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTestCausalLinearAttention\u001b[39;00m(\u001b[43munittest\u001b[49m\u001b[38;5;241m.\u001b[39mTestCase):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, S\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, H\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, E\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, D\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m             torch\u001b[38;5;241m.\u001b[39mrand(N, L, H, E)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m      5\u001b[0m             torch\u001b[38;5;241m.\u001b[39mrand(N, S, H, E)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m             FullMask(N, S, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     10\u001b[0m         )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unittest' is not defined"
     ]
    }
   ],
   "source": [
    "class TestCausalLinearAttention(unittest.TestCase):\n",
    "    def _get_inputs(self, N=10, L=5, S=8, H=4, E=32, D=64, device=\"cpu\"):\n",
    "        return (\n",
    "            torch.rand(N, L, H, E).to(device),\n",
    "            torch.rand(N, S, H, E).to(device),\n",
    "            torch.rand(N, S, H, D).to(device),\n",
    "            TriangularCausalMask(L, device=device),\n",
    "            FullMask(N, L, device=device),\n",
    "            FullMask(N, S, device=device)\n",
    "        )\n",
    "\n",
    "    def test_forward(self):\n",
    "        att = CausalLinearAttention(32)\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs(L=5, S=5)\n",
    "        v = att(q, k, v, m1, m2, m3)\n",
    "        self.assertTrue(v.is_contiguous())\n",
    "\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs(L=5, S=10)\n",
    "        v = att(q, k, v, m1, m2, m3)\n",
    "        self.assertTrue(v.is_contiguous())\n",
    "\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs(L=10, S=5)\n",
    "        v = att(q, k, v, m1, m2, m3)\n",
    "        self.assertTrue(v.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970b5fd-ef64-4165-b7b9-40cb68a12d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140da3a-69ba-4fad-bc65-530a98197fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4137cad2-f725-470c-b573-63e18c8b2a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e2dff-9e37-4184-9a1f-1edc1703e112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07697734-25cd-4827-a2fe-3156835e48e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28834e9-2cec-46ca-99a6-f4592e3206d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d8a3b-51c7-4144-90f8-3f15fb598810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac2e3c-f200-4b52-8827-aeeaf6c044f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpt",
   "language": "python",
   "name": "jpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
