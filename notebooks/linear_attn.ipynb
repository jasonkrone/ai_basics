{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6836f9-050a-4f5a-956e-bdcf32e8f26d",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "https://github.com/idiap/fast-transformers/tree/2ad36b97e64cb93862937bd21fcc9568d989561f/fast_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94c5caab-08f4-442a-a852-1cbaddf56dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2d62b-38fd-4418-bac7-80dfc5b49e65",
   "metadata": {},
   "source": [
    "### Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de6e1a7-36e8-468c-acd3-661ef8822bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseMask(object):\n",
    "    @property\n",
    "    def bool_matrix(self):\n",
    "        \"\"\"Return a bool (uint8) matrix with 1s to all places that should be\n",
    "        kept.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def float_matrix(self):\n",
    "        \"\"\"Return the bool matrix as a float to be used as a multiplicative\n",
    "        mask for non softmax attentions.\"\"\"\n",
    "        if not hasattr(self, \"_float_matrix\"):\n",
    "            with torch.no_grad():\n",
    "                self._float_matrix = self.bool_matrix.float()\n",
    "        return self._float_matrix\n",
    "\n",
    "    @property\n",
    "    def lengths(self):\n",
    "        \"\"\"If the matrix is of the following form\n",
    "\n",
    "            1 1 1 0 0 0 0\n",
    "            1 0 0 0 0 0 0\n",
    "            1 1 0 0 0 0 0\n",
    "\n",
    "        then return it as a vector of integers\n",
    "\n",
    "            3 1 2.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"_lengths\"):\n",
    "            with torch.no_grad():\n",
    "                lengths = self.bool_matrix.long().sum(dim=-1)\n",
    "                # make sure that the mask starts with 1s and continues with 0s\n",
    "                # this should be changed to something more efficient, however,\n",
    "                # I chose simplicity over efficiency since the LengthMask class\n",
    "                # will be used anyway (and the result is cached)\n",
    "                m = self.bool_matrix.view(-1, self.shape[-1])\n",
    "                for i, l in enumerate(lengths.view(-1)):\n",
    "                    if not torch.all(m[i, :l]):\n",
    "                        raise ValueError(\"The mask is not a length mask\")\n",
    "                self._lengths = lengths\n",
    "        return self._lengths\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        \"\"\"Return the shape of the boolean mask.\"\"\"\n",
    "        return self.bool_matrix.shape\n",
    "\n",
    "    @property\n",
    "    def additive_matrix(self):\n",
    "        \"\"\"Return a float matrix to be added to an attention matrix before\n",
    "        softmax.\"\"\"\n",
    "        if not hasattr(self, \"_additive_matrix\"):\n",
    "            with torch.no_grad():\n",
    "                self._additive_matrix = torch.log(self.bool_matrix.float())\n",
    "        return self._additive_matrix\n",
    "\n",
    "    @property\n",
    "    def additive_matrix_finite(self):\n",
    "        \"\"\"Same as additive_matrix but with -1e24 instead of infinity.\"\"\"\n",
    "        if not hasattr(self, \"_additive_matrix_finite\"):\n",
    "            with torch.no_grad():\n",
    "                self._additive_matrix_finite = (\n",
    "                    (~self.bool_matrix).float() * (-1e24)\n",
    "                )\n",
    "        return self._additive_matrix_finite\n",
    "\n",
    "    @property\n",
    "    def all_ones(self):\n",
    "        \"\"\"Return true if the mask is all ones.\"\"\"\n",
    "        if not hasattr(self, \"_all_ones\"):\n",
    "            with torch.no_grad():\n",
    "                self._all_ones = torch.all(self.bool_matrix)\n",
    "        return self._all_ones\n",
    "\n",
    "    @property\n",
    "    def lower_triangular(self):\n",
    "        \"\"\"Return true if the attention is a triangular causal mask.\"\"\"\n",
    "        if not hasattr(self, \"_lower_triangular\"):\n",
    "            self._lower_triangular = False\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    lengths = self.lengths\n",
    "                    if len(lengths.shape) == 1:\n",
    "                        target = torch.arange(\n",
    "                            1,\n",
    "                            len(lengths)+1,\n",
    "                            device=lengths.device\n",
    "                        )\n",
    "                        self._lower_triangular = torch.all(lengths == target)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        return self._lower_triangular\n",
    "\n",
    "\n",
    "class FullMask(BaseMask):\n",
    "    \"\"\"Thin wrapper over a pytorch tensor that provides the BaseMask\n",
    "    interface.\n",
    "\n",
    "    The arguments can be given both by keyword arguments and positional\n",
    "    arguments. To imitate function overloading, the constructor checks the type\n",
    "    of the first argument and if it is a tensor it treats it as the mask.\n",
    "    otherwise it assumes that it was the N argument.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        mask: The mask as a PyTorch tensor.\n",
    "        N: The rows of the all True mask to be created if the mask argument is\n",
    "           not provided.\n",
    "        M: The columns of the all True mask to be created if the mask argument\n",
    "           is not provided. If N is given M defaults to N.\n",
    "        device: The device to create the mask in (defaults to cpu)\n",
    "    \"\"\"\n",
    "    def __init__(self, mask=None, N=None, M=None, device=\"cpu\"):\n",
    "        # mask is a tensor so we ignore N and M\n",
    "        if mask is not None and isinstance(mask, torch.Tensor):\n",
    "            if mask.dtype != torch.bool:\n",
    "                raise ValueError(\"FullMask expects the mask to be bool\")\n",
    "            with torch.no_grad():\n",
    "                self._mask = mask.clone()\n",
    "            return\n",
    "\n",
    "        # mask is an integer, N is an integer and M is None so assume they were\n",
    "        # passed as N, M\n",
    "        if mask is not None and M is None and isinstance(mask, int):\n",
    "            M = N\n",
    "            N = mask\n",
    "\n",
    "        if N is not None:\n",
    "            M = M or N\n",
    "            with torch.no_grad():\n",
    "                self._mask = torch.ones(N, M, dtype=torch.bool, device=device)\n",
    "            self._all_ones = True\n",
    "            return\n",
    "\n",
    "        raise ValueError(\"Either mask or N should be provided\")\n",
    "\n",
    "    @property\n",
    "    def bool_matrix(self):\n",
    "        return self._mask\n",
    "\n",
    "\n",
    "class LengthMask(BaseMask):\n",
    "    \"\"\"Provide a BaseMask interface for lengths. Mostly to be used with\n",
    "    sequences of different lengths.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        lengths: The lengths as a PyTorch long tensor\n",
    "        max_len: The maximum length for the mask (defaults to lengths.max())\n",
    "        device: The device to be used for creating the masks (defaults to\n",
    "                lengths.device)\n",
    "    \"\"\"\n",
    "    def __init__(self, lengths, max_len=None, device=None):\n",
    "        self._device = device or lengths.device\n",
    "        with torch.no_grad():\n",
    "            self._lengths = lengths.clone().to(self._device)\n",
    "        self._max_len = max_len or self._lengths.max()\n",
    "\n",
    "        self._bool_matrix = None\n",
    "        self._all_ones = torch.all(self._lengths == self._max_len).item()\n",
    "\n",
    "    @property\n",
    "    def bool_matrix(self):\n",
    "        if self._bool_matrix is None:\n",
    "            with torch.no_grad():\n",
    "                indices = torch.arange(self._max_len, device=self._device)\n",
    "                self._bool_matrix = (\n",
    "                    indices.view(1, -1) < self._lengths.view(-1, 1)\n",
    "                )\n",
    "        return self._bool_matrix\n",
    "\n",
    "\n",
    "class TriangularCausalMask(LengthMask):\n",
    "    \"\"\"A square matrix with everything masked out above the diagonal.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        N: The size of the matrix\n",
    "        device: The device to create the mask in (defaults to cpu)\n",
    "    \"\"\"\n",
    "    def __init__(self, N, device=\"cpu\"):\n",
    "        lengths = torch.arange(1, N+1, device=device)\n",
    "        super(TriangularCausalMask, self).__init__(lengths, N, device)\n",
    "        self._lower_triangular = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc860e-b428-4a47-914f-af843881e618",
   "metadata": {},
   "source": [
    "### ELU Feature Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7298fee-bf61-4163-907f-811e1201406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class FeatureMap(Module):\n",
    "    \"\"\"Define the FeatureMap interface.\"\"\"\n",
    "    def __init__(self, query_dims):\n",
    "        super().__init__()\n",
    "        self.query_dims = query_dims\n",
    "\n",
    "    def new_feature_map(self, device):\n",
    "        \"\"\"Create a new instance of this feature map. In particular, if it is a\n",
    "        random feature map sample new parameters.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward_queries(self, x):\n",
    "        \"\"\"Encode the queries `x` using this feature map.\"\"\"\n",
    "        return self(x)\n",
    "\n",
    "    def forward_keys(self, x):\n",
    "        \"\"\"Encode the keys `x` using this feature map.\"\"\"\n",
    "        return self(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Encode x using this feature map. For symmetric feature maps it\n",
    "        suffices to define this function, but for asymmetric feature maps one\n",
    "        needs to define the `forward_queries` and `forward_keys` functions.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def factory(cls, *args, **kwargs):\n",
    "        \"\"\"Return a function that when called with the query dimensions returns\n",
    "        an instance of this feature map.\n",
    "\n",
    "        It is inherited by the subclasses so it is available in all feature\n",
    "        maps.\n",
    "        \"\"\"\n",
    "        def inner(query_dims):\n",
    "            return cls(query_dims, *args, **kwargs)\n",
    "        return inner\n",
    "\n",
    "\n",
    "class ActivationFunctionFeatureMap(FeatureMap):\n",
    "    \"\"\"Define a feature map that is simply an element-wise activation\n",
    "    function.\"\"\"\n",
    "    def __init__(self, query_dims, activation_function):\n",
    "        super().__init__(query_dims)\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def new_feature_map(self, device):\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation_function(x)\n",
    "\n",
    "\n",
    "elu_feature_map = ActivationFunctionFeatureMap.factory(\n",
    "    lambda x: torch.nn.functional.elu(x) + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d60256-ac8f-4dad-98c4-4fd285ec583c",
   "metadata": {},
   "source": [
    "### Linear Attention Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b232073-9484-4c5d-bd15-1d8edf960881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttention(Module):\n",
    "    \"\"\"Implement unmasked attention using dot product of feature maps in\n",
    "    O(N D^2) complexity.\n",
    "\n",
    "    Given the queries, keys and values as Q, K, V instead of computing\n",
    "\n",
    "        V' = softmax(Q.mm(K.t()), dim=-1).mm(V),\n",
    "\n",
    "    we make use of a feature map function Φ(.) and perform the following\n",
    "    computation\n",
    "\n",
    "        V' = normalize(Φ(Q).mm(Φ(K).t())).mm(V).\n",
    "\n",
    "    The above can be computed in O(N D^2) complexity where D is the\n",
    "    dimensionality of Q, K and V and N is the sequence length. Depending on the\n",
    "    feature map, however, the complexity of the attention might be limited.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        feature_map: callable, a callable that applies the feature map to the\n",
    "                     last dimension of a tensor (default: elu(x)+1)\n",
    "        eps: float, a small number to ensure the numerical stability of the\n",
    "             denominator (default: 1e-6)\n",
    "        event_dispatcher: str or EventDispatcher instance to be used by this\n",
    "                          module for dispatching events (default: the default\n",
    "                          global dispatcher)\n",
    "    \"\"\"\n",
    "    def __init__(self, query_dimensions, feature_map=None, eps=1e-6, event_dispatcher=\"\"):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        self.feature_map = elu_feature_map(query_dimensions)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, query_lengths, key_lengths):\n",
    "        \"\"\"\n",
    "        queries: (N, L, H, E)\n",
    "        keys: (N, S, H, E)\n",
    "        values: (N, S, H, D)\n",
    "        attn_mask: (L, S)\n",
    "        query_lengths: (N, L)\n",
    "        key_lengths: (N, S)\n",
    "\n",
    "        where\n",
    "            - N: batch-size\n",
    "            - L: seq len for queries\n",
    "            - S: seq len for keys & values\n",
    "            - H: number of heads\n",
    "            - E: key & query dim\n",
    "            - D: value dim\n",
    "        \"\"\"\n",
    "        # Apply the feature map to the queries and keys\n",
    "        self.feature_map.new_feature_map(queries.device)\n",
    "        Q = self.feature_map.forward_queries(queries)\n",
    "        K = self.feature_map.forward_keys(keys)\n",
    "\n",
    "        # Apply the key padding mask and make sure that the attn_mask is\n",
    "        # all_ones\n",
    "        if not attn_mask.all_ones:\n",
    "            raise RuntimeError((\"LinearAttention does not support arbitrary attention masks\"))\n",
    "\n",
    "        # key_lengths => (N, S, 1, 1)\n",
    "        K = K * key_lengths.float_matrix[:, :, None, None]\n",
    "\n",
    "        # Compute the KV matrix, namely the dot product of keys and values so\n",
    "        # that we never explicitly compute the attention matrix and thus\n",
    "        # decrease the complexity\n",
    "        KV = torch.einsum(\"nshd,nshm->nhmd\", K, values)\n",
    "\n",
    "        # Compute the normalizer\n",
    "        Z = 1/(torch.einsum(\"nlhd,nhd->nlh\", Q, K.sum(dim=1))+self.eps)\n",
    "\n",
    "        # Finally compute and return the new values\n",
    "        V = torch.einsum(\"nlhd,nhmd,nlh->nlhm\", Q, KV, Z)\n",
    "\n",
    "        return V.contiguous(), KV, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5016ca-9303-4b71-8efb-e07ae2897d60",
   "metadata": {},
   "source": [
    "### Tester Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40faa5ce-cda5-4ac0-8abb-bea6027be1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TestLinearAttention(object): #(unittest.TestCase):\n",
    "    \n",
    "    def _get_inputs(self, N=10, L=5, S=8, H=4, E=32, D=64, device=\"cpu\"):\n",
    "        return (\n",
    "            torch.rand(N, L, H, E).to(device), # Q\n",
    "            torch.rand(N, S, H, E).to(device), # K\n",
    "            torch.rand(N, S, H, D).to(device), # V\n",
    "            FullMask(L, S, device=device), # m1\n",
    "            FullMask(N, L, device=device), # m2\n",
    "            FullMask(N, S, device=device) # m3\n",
    "        )\n",
    "\n",
    "    # TODO: JPK added\n",
    "    def get_their_forward(self):\n",
    "        att = LinearAttention(32)\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs()\n",
    "        v = att(q, k, v, m1, m2, m3)\n",
    "        return v\n",
    "    \n",
    "    def test_forward(self):\n",
    "        att = LinearAttention(32)\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs()\n",
    "        v = att(q, k, v, m1, m2, m3)\n",
    "        self.assertTrue(v.is_contiguous())\n",
    "\n",
    "    def test_masking(self):\n",
    "        att = LinearAttention(32)\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs()\n",
    "\n",
    "        # Make sure that we raise an error if m1 is not all ones\n",
    "        with self.assertRaises(RuntimeError):\n",
    "            att(q, k, v, FullMask(torch.rand(*m1.shape) > 0.5), m2, m3)\n",
    "\n",
    "        # Make sure that the key lengths is paid attention to\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs(S=10, D=1)\n",
    "        m3 = LengthMask(torch.tensor(list(range(10)))+1)\n",
    "        for i in range(9):\n",
    "            v[i, i+1:] = 1e9\n",
    "        v_new = att(q, k, v, m1, m2, m3)\n",
    "        self.assertLess(v_new.max().item(), 1)\n",
    "\n",
    "    def test_benchmark_cpu(self):\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs(L=1024, S=1024, E=64, D=64)\n",
    "        att = LinearAttention(64)\n",
    "\n",
    "        # warmup the cache\n",
    "        for i in range(10):\n",
    "            v_new = att(q, k, v, m1, m2, m3)\n",
    "\n",
    "        # measure\n",
    "        start = time.time()\n",
    "        for i in range(10):\n",
    "            v_new = att(q, k, v, m1, m2, m3)\n",
    "        end = time.time()\n",
    "        print(\"CPU time taken:\", (end-start)*1000, \"(ms)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6542bc-db50-458d-bada-c209e8094d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyLinearAttention(Module):\n",
    "    \"\"\"Implement unmasked attention using dot product of feature maps in\n",
    "    O(N D^2) complexity.\n",
    "\n",
    "    Given the queries, keys and values as Q, K, V instead of computing\n",
    "\n",
    "        V' = softmax(Q.mm(K.t()), dim=-1).mm(V),\n",
    "\n",
    "    we make use of a feature map function Φ(.) and perform the following\n",
    "    computation\n",
    "\n",
    "        V' = normalize(Φ(Q).mm(Φ(K).t())).mm(V).\n",
    "\n",
    "    The above can be computed in O(N D^2) complexity where D is the\n",
    "    dimensionality of Q, K and V and N is the sequence length. Depending on the\n",
    "    feature map, however, the complexity of the attention might be limited.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        feature_map: callable, a callable that applies the feature map to the\n",
    "                     last dimension of a tensor (default: elu(x)+1)\n",
    "        eps: float, a small number to ensure the numerical stability of the\n",
    "             denominator (default: 1e-6)\n",
    "        event_dispatcher: str or EventDispatcher instance to be used by this\n",
    "                          module for dispatching events (default: the default\n",
    "                          global dispatcher)\n",
    "    \"\"\"\n",
    "    def __init__(self, query_dimensions, feature_map=None, eps=1e-6, event_dispatcher=\"\"):\n",
    "        super(MyLinearAttention, self).__init__()\n",
    "        self.feature_map = elu_feature_map(query_dimensions)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, query_lengths, key_lengths):\n",
    "        \"\"\"\n",
    "        queries: (N, L, H, E)\n",
    "        keys: (N, S, H, E)\n",
    "        values: (N, S, H, D)\n",
    "        attn_mask: (L, S)\n",
    "        query_lengths: (N, L)\n",
    "        key_lengths: (N, S)\n",
    "\n",
    "        where\n",
    "            - N: batch-size\n",
    "            - L: seq len for queries\n",
    "            - S: seq len for keys & values\n",
    "            - H: number of heads\n",
    "            - E: key & query dim\n",
    "            - D: value dim\n",
    "        \"\"\"\n",
    "        # Apply the feature map to the queries and keys\n",
    "        self.feature_map.new_feature_map(queries.device)\n",
    "        Q = self.feature_map.forward_queries(queries)\n",
    "        K = self.feature_map.forward_keys(keys)\n",
    "        # change the shapes so we broadcast across the right dims\n",
    "        N, L, H, E = Q.shape\n",
    "        _, S, _, _ = K.shape\n",
    "\n",
    "        # (N, L, H, E) => (N, H, L, E)\n",
    "        Q = Q.transpose(1, 2)\n",
    "        # (N, S, H, E) => (N, H, S, E)\n",
    "        K = K.transpose(1, 2)\n",
    "        # (N, S, H, D) => (N, H, S, D)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # reshape K & V to get KV\n",
    "        # (N, H, S, E) => (N, H, S, E, 1)\n",
    "        K = K[:, :, :, :, None]\n",
    "        # (N, H, S, D) => (N, H, S, 1, D)\n",
    "        values = values[:, :, :, None, :]\n",
    "\n",
    "        # (N, H, S, E, 1) x (N, H, S, 1, D) = (N, H, E, D)\n",
    "        KV = torch.sum(K @ values, dim=2)\n",
    "        # (N, H, L, E) x (N, H, E, D) = (N, H, L, D) \n",
    "        QKV = Q @ KV \n",
    "        # (N, H, E)\n",
    "        K_sum = torch.sum(K, dim=2).squeeze()\n",
    "        # (N, H, L, E) x (N, H, E, 1) = > (N, H, L, 1) \n",
    "        Z = 1 / (Q @ K_sum[:, :, :, None] + self.eps).squeeze()\n",
    "        # (N, H, L, D) x (N, H, L, 1) = (N, H, L, D) \n",
    "        out = QKV * Z[:, :, :, None]\n",
    "        # (N, H, L, D) => (N, L, H, D)\n",
    "        out = out.transpose(1, 2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "49da8956-6ecb-44b2-905f-dcc4e339a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EinsumLinearAttention(Module):\n",
    "    \n",
    "    def __init__(self, query_dimensions, feature_map=None, eps=1e-6, event_dispatcher=\"\"):\n",
    "        super().__init__()\n",
    "        self.feature_map = elu_feature_map(query_dimensions)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, query_lengths, key_lengths):\n",
    "        \"\"\"\n",
    "        queries: (N, L, H, E)\n",
    "        keys: (N, S, H, E)\n",
    "        values: (N, S, H, D)\n",
    "        attn_mask: (L, S)\n",
    "        query_lengths: (N, L)\n",
    "        key_lengths: (N, S)\n",
    "\n",
    "        where\n",
    "            - N: batch-size\n",
    "            - L: seq len for queries\n",
    "            - S: seq len for keys & values\n",
    "            - H: number of heads\n",
    "            - E: key & query dim\n",
    "            - D: value dim\n",
    "        \"\"\"\n",
    "        # Apply the feature map to the queries and keys\n",
    "        self.feature_map.new_feature_map(queries.device)\n",
    "        Q = self.feature_map.forward_queries(queries)\n",
    "        K = self.feature_map.forward_keys(keys)\n",
    "        # (N, H, E, D)\n",
    "        KV_sum = torch.einsum(\"nshi,nshj->nhij\", [K, values])\n",
    "        # (N, L, H, D)\n",
    "        scores = torch.einsum(\"nthk,nhkd->nthd\", [Q, KV_sum])        \n",
    "        z = 1 / (torch.einsum(\"nthe,nhe->nth\",[Q, K.sum(dim=1)]) + self.eps)\n",
    "        out = scores * z[:, :, :, None]\n",
    "        return out, KV_sum, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c2719-0008-4e67-9eee-4d2cb37efd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f91aa-491d-470a-bfd3-09b7dd82efc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "07ce78ae-9063-40ba-9287-a435ff7a1d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_temp = torch.rand(10, 4, 5, 64)\n",
    "z_temp = torch.rand(10, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dbe45afd-c1c4-4790-9291-f88bf0f35753",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_tester = TestLinearAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "afc3c149-4b63-47b0-b30e-54e445f334b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v, m1, m2, m3 = lin_tester._get_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "41bbe50e-f319-4039-9693-e3912c3647fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mine = MyLinearAttention(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ef5619e3-a769-4836-a131-6a3d6fc395dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "theirs = LinearAttention(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5b454ee2-bf0d-4a57-b888-dd8d86974513",
   "metadata": {},
   "outputs": [],
   "source": [
    "ein = EinsumLinearAttention(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "81719ef4-72d6-4c29-838e-3abe55737a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_out = mine(q, k, v, m1, m2, m3)\n",
    "their_out, their_kv, their_z = theirs(q, k, v, m1, m2, m3)\n",
    "ein_out, ein_kv, ein_z = ein(q, k, v, m1, m2, m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959929d3-672e-4451-bc09-62d4c566ef85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b66e07bb-50e5-4ca1-a923-d128ae133449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(my_out, their_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b67fe190-5b4d-4403-bdf9-b488e07bc628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(ein_out, their_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "09149244-0e10-4cad-852d-3dafb51af6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ein_out - their_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f0fa7-caf5-4c17-b4f3-89f319aafcc1",
   "metadata": {},
   "source": [
    "### Linear Attention Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22a454-928d-4806-ae29-f42871d7bb99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab72cd-cb01-47db-beb4-737aae4524fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f02abb-6a8e-4786-8b04-7908bc7b0156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c1df0cd-bc09-4b56-a29d-2aac1e7a42f1",
   "metadata": {},
   "source": [
    "### Causal Linear Attention Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d07f1178-6b4d-4824-8b14-9026fcbede33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalLinearAttention(Module):\n",
    "    \"\"\"Implement causally masked attention using dot product of feature maps in\n",
    "    O(N D^2) complexity.\n",
    "\n",
    "    See fast_transformers.attention.linear_attention.LinearAttention for the\n",
    "    general concept of replacing the softmax with feature maps. In addition to\n",
    "    that, we also make use of the fact that causal masking is a triangular mask\n",
    "    which allows us to apply the masking and still compute the attention in O(N\n",
    "    D^2) complexity.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "        feature_map: callable, a callable that applies the feature map to the\n",
    "                     last dimension of a tensor (default: elu(x)+1)\n",
    "        eps: float, a small number to ensure the numerical stability of the\n",
    "             denominator (default: 1e-6)\n",
    "        event_dispatcher: str or EventDispatcher instance to be used by this\n",
    "                          module for dispatching events (default: the default\n",
    "                          global dispatcher)\n",
    "    \"\"\"\n",
    "    def __init__(self, query_dimensions, feature_map=None, eps=1e-6,\n",
    "                 event_dispatcher=\"\"):\n",
    "        super(CausalLinearAttention, self).__init__()\n",
    "        self.feature_map = (\n",
    "            feature_map(query_dimensions) if feature_map else\n",
    "            elu_feature_map(query_dimensions)\n",
    "        )\n",
    "        self.eps = eps\n",
    "\n",
    "    def _make_sizes_compatible(self, Q, K):\n",
    "        \"\"\"Either slice or pad K in case that the sizes do not match between Q\n",
    "        and K.\"\"\"\n",
    "        N, L, H, E = Q.shape\n",
    "        _, S, _, _ = K.shape\n",
    "        if L == S:\n",
    "            return Q, K\n",
    "\n",
    "        if L < S:\n",
    "            return Q, K[:, :L, :, :]\n",
    "\n",
    "        if L > S:\n",
    "            return Q, torch.cat([K, K.new_zeros(N, L-S, H, E)], dim=1)\n",
    "\n",
    "    def _causal_linear(self, Q, K, V):\n",
    "        Q = Q.permute(0,2,1,3).contiguous()\n",
    "        K = K.permute(0,2,1,3).contiguous()\n",
    "        V = V.permute(0,2,1,3).contiguous()\n",
    "        V_new = self._causal_dot_product(Q, K, V)\n",
    "        return V_new.permute(0,2,1,3).contiguous()\n",
    "   \n",
    "    def _causal_dot_product(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        Written by claude to make this fn pytorch vs. cpp so may not be correct\n",
    "        https://github.com/idiap/fast-transformers/blob/2ad36b97e64cb93862937bd21fcc9568d989561f/fast_transformers/causal_product/causal_product_cpu.cpp\n",
    "        \"\"\"\n",
    "        # Extract shapes\n",
    "        batch_size, num_heads, seq_len, head_dim = queries.shape\n",
    "        value_dim = values.shape[-1]\n",
    "        \n",
    "        # Initialize output tensor\n",
    "        product = torch.zeros(batch_size, num_heads, seq_len, value_dim, \n",
    "                             device=queries.device, dtype=queries.dtype)\n",
    "        \n",
    "        # Iterate through sequence positions\n",
    "        for i in range(seq_len):\n",
    "            # For each position i, we only consider keys and values up to position i (inclusive)\n",
    "            # This enforces the causal masking\n",
    "            q_i = queries[:, :, i:i+1, :]  # Shape: [B, H, 1, E]\n",
    "            k_0_to_i = keys[:, :, :i+1, :]  # Shape: [B, H, i+1, E]\n",
    "            v_0_to_i = values[:, :, :i+1, :]  # Shape: [B, H, i+1, M]\n",
    "            \n",
    "            # Compute Q_i * K_{0:i}^T -> [B, H, 1, i+1]\n",
    "            attention = torch.matmul(q_i, k_0_to_i.transpose(-1, -2))\n",
    "            \n",
    "            # Apply attention weights to values V_{0:i}\n",
    "            # [B, H, 1, i+1] @ [B, H, i+1, M] -> [B, H, 1, M]\n",
    "            product[:, :, i:i+1, :] = torch.matmul(attention, v_0_to_i)\n",
    "        \n",
    "        return product\n",
    "    \n",
    "    def forward(self, queries, keys, values, attn_mask, query_lengths,\n",
    "                key_lengths):\n",
    "        # Apply the feature map to the queries and keys\n",
    "        self.feature_map.new_feature_map(queries.device)\n",
    "        Q = self.feature_map.forward_queries(queries)\n",
    "        K = self.feature_map.forward_keys(keys)\n",
    "\n",
    "        # Apply the key padding mask and make sure the attn_mask is a\n",
    "        # lower triangular causal mask\n",
    "        if not attn_mask.lower_triangular:\n",
    "            raise RuntimeError((\"CausalLinearAttention only supports full \"\n",
    "                                \"lower triangular masks\"))\n",
    "\n",
    "        # what's this do \n",
    "        K = K * key_lengths.float_matrix[:, :, None, None]\n",
    "\n",
    "        # Ensure that Q and K have compatible sizes for the following\n",
    "        # computations, namely L == S\n",
    "        Q, K = self._make_sizes_compatible(Q, K)\n",
    "\n",
    "        \"\"\"\n",
    "        think the above is just dealing w/ the case that Len of and K differ\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Shall we divide the Q and K with a relatively large number to\n",
    "        #       avoid numerical instabilities in computing the denominator?\n",
    "        #       We used to divide each with the max norm of all q and k but\n",
    "        #       that seems relatively costly for a simple normalization.\n",
    "\n",
    "        # Compute the normalizers\n",
    "        Z = 1/(torch.einsum(\"nlhi,nlhi->nlh\", Q, K.cumsum(1)) + self.eps)\n",
    "\n",
    "        # Compute the unnormalized result\n",
    "        V = self._causal_linear(\n",
    "            Q,\n",
    "            K,\n",
    "            values\n",
    "        )\n",
    "\n",
    "        return V * Z[:, :, :, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c37d8-883f-4981-9383-78f15e3aead8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb914199-8b9d-40ae-98f4-aab471284d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b36c604-e114-44d7-8d44-64393885a01c",
   "metadata": {},
   "source": [
    "### Causal Linear Attention Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "17c867eb-7b2e-4d07-85b3-6209f47d841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCausalLinearAttention(object):\n",
    "    def _get_inputs(self, N=10, L=5, S=8, H=4, E=32, D=64, device=\"cpu\"):\n",
    "        return (\n",
    "            torch.rand(N, L, H, E).to(device),\n",
    "            torch.rand(N, S, H, E).to(device),\n",
    "            torch.rand(N, S, H, D).to(device),\n",
    "            TriangularCausalMask(L, device=device),\n",
    "            FullMask(N, L, device=device),\n",
    "            FullMask(N, S, device=device)\n",
    "        )\n",
    "\n",
    "    def test_forward(self):\n",
    "        att = CausalLinearAttention(32)\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs(L=5, S=5)\n",
    "        v = att(q, k, v, m1, m2, m3)\n",
    "        assert v.is_contiguous()\n",
    "\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs(L=5, S=10)\n",
    "        v = att(q, k, v, m1, m2, m3)\n",
    "        assert v.is_contiguous()\n",
    "\n",
    "        q, k, v, m1, m2, m3 = self._get_inputs(L=10, S=5)\n",
    "        v = att(q, k, v, m1, m2, m3)\n",
    "        assert v.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae6478-a470-4e33-9622-35628e138481",
   "metadata": {},
   "source": [
    "### My Causal Linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3b1cc5a3-f806-412f-817a-82c1e1c7a4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EinsumCausalLinearAttention(Module):\n",
    "    \n",
    "    def __init__(self, query_dimensions, feature_map=None, eps=1e-6, event_dispatcher=\"\"):\n",
    "        super().__init__()\n",
    "        self.feature_map = elu_feature_map(query_dimensions)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, query_lengths, key_lengths):\n",
    "        \"\"\"\n",
    "        queries: (N, L, H, E)\n",
    "        keys: (N, S, H, E)\n",
    "        values: (N, S, H, D)\n",
    "        attn_mask: (L, S)\n",
    "        query_lengths: (N, L)\n",
    "        key_lengths: (N, S)\n",
    "\n",
    "        where\n",
    "            - N: batch-size\n",
    "            - L: seq len for queries\n",
    "            - S: seq len for keys & values\n",
    "            - H: number of heads\n",
    "            - E: key & query dim\n",
    "            - D: value dim\n",
    "        \"\"\"\n",
    "        self.feature_map.new_feature_map(queries.device)\n",
    "        Q = self.feature_map.forward_queries(queries)\n",
    "        K = self.feature_map.forward_keys(keys)\n",
    "\n",
    "        # compute the denom via cum sum \n",
    "        # (N, S, H, E) \n",
    "        K_cumsum = K.cumsum(dim=1)\n",
    "        # (N, L, H) \n",
    "        Z = 1 / (torch.einsum(\"bihj,bihj->bih\", [Q, K_cumsum]) + self.eps)\n",
    "        num = self._causal_linear(Q, K, values)\n",
    "        out = num * Z[:, :, :, None]\n",
    "        return out\n",
    "\n",
    "    def _causal_linear(self, Q, K, V):\n",
    "        \"\"\"\n",
    "        We assume that seq lens are all the same here\n",
    "\n",
    "        Q: (N, L, H, E)\n",
    "        K: (N, L, H, E)\n",
    "        V: (N, L, H, D)\n",
    "        \"\"\"\n",
    "        out = torch.zeros_like(V)\n",
    "        N, L, H, D = V.shape\n",
    "        _, _, _, E = K.shape\n",
    "        \n",
    "        # (N, H, E, D) \n",
    "        S_prev = torch.zeros(N, H, E, D)\n",
    "\n",
    "        for i in range(L):\n",
    "            # (N, H, E, D)\n",
    "            S_i = torch.einsum(\"nhj,nhk->nhjk\", [K[:, i, :, :], V[:, i, :, :]]).squeeze() + S_prev\n",
    "            S_prev = S_i\n",
    "            # (N, H, E), (N, H, E, D) => (N, 1, H, D)\n",
    "            out[:, i, :, :] = torch.einsum(\"nhj,nhjk->nhk\", [Q[:, i, :, :], S_i])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "18ddd4d5-334c-4a3c-9ff0-8e60af24e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_tester = TestCausalLinearAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "04ba02c5-2164-4e73-a8a3-cfb1e53b71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_caus, k_caus, v_caus, m1_caus, m2_caus, m3_caus = causal_tester._get_inputs(\n",
    "    N=10, L=8, S=8, H=4, E=32, D=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "c970b5fd-ef64-4165-b7b9-40cb68a12d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "their_causal_linear = CausalLinearAttention(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "95b22ec1-b1a7-493f-853b-ab7d63b88a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_causal_linear = EinsumCausalLinearAttention(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "b606a85f-f1cf-4f50-9abb-5a470d59797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "their_causal_out = their_causal_linear(q_caus, k_caus, v_caus, m1_caus, m2_caus, m3_caus)\n",
    "my_causal_out = my_causal_linear(q_caus, k_caus, v_caus, m1_caus, m2_caus, m3_caus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "07697734-25cd-4827-a2fe-3156835e48e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(their_causal_out, my_causal_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac2e3c-f200-4b52-8827-aeeaf6c044f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpt",
   "language": "python",
   "name": "jpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
